---
title: "Case Study 3: Galeries Lafayette"
author: "UNIGE - GSEM - Advanced Data-Driven Decision Making"
date: "`r Sys.Date()`"
abstract:
  Identify the key drivers of brand equity for Galeries Lafayette based on a questionnaire mailed to 5000 customers and returned by 600 of them
output: 
  html_document: 
    theme: readable
    highlight: pygments
    toc: true
    toc_depth: 6
    toc_float:
      collapsed: false
      smooth_scroll: false
    number_sections: T
---

```{css, echo=FALSE}
  #TOC {
    max-width: fit-content;
    white-space: nowrap;
  }
  
  div:has(> #TOC) {
    display: flex;
    flex-direction: row-reverse;
}
    ```

> *This analysis was prepared by Francisco Arrieta and Jonathan Edwards.*

------------------------------------------------------------------------

## Setup

```{r knitr setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE) #display source code in output
knitr::opts_chunk$set(message = FALSE, warning = FALSE) #display warnings and error messages
```

```{r cleardata, include=FALSE}
rm(list=ls()) # clear the data
```

```{r}
# options(scipen=999) #prevent scientific notation
# options(scipen=-999) #encourage scientific notation
options(scipen=0) #encourage scientific notation neutral?
```


## load data


```{r activate libraries}
# modelling
library(psych) #factor analysis tools (PCA PAF)
library(lavaan) #causal analysis
library(lm.beta) # add standarized regression coeffs

# stats
library(nortest) #Kolmogorov-Smirnov-Test
library(corrplot) #correlation matrix plot
library(olsrr)  #VIF and Tolerance Values
library(pastecs) # provides function stat.desc
library(REdaS) #Bartelett's Test

# plotting & formatting
library(ggplot2) #better graphs
library(patchwork) # provides wrap_plots for multiplotting 
# library(gridExtra) #provides multiplotting functionality
# library(ggpubr) #provides ggarrange for multiplotting (patchwork better though)
library(semPlot) #for visualization of path diagrams (SEM)
library(lavaanPlot) #for visualization of path diagrams (SEM)
# library(rcompanion)   #Histogram and Normal Curve
library(kableExtra) #makes nice tables

# generic
library(dplyr) #useful data manip functions like arrange, distinct, rename etc included in fpp3
library(stringr) # provides string manip functions like str_split_fixed 
library(Hmisc) #describe function that describes features of dataframes
library(data.table) # creating and manipulating datatables
library(knitr) #rmarkdown tools not sure why useful
library(parameters) #get model outputs in table form (good for making tabs)
```

## Load data

```{r load datasets}
survey <- read.csv("Case Study III_Structural Equation Modeling.csv")
labels <- read.csv("Variables and Labels_Galeries Lafayette.csv")

survey
```


## Clean and handle missing data

```{r}
# # omit all unanswered
# filter_all(survey, all_vars(. != 999))
# filter_all(survey, any_vars(. %in% c(999)))
# 
# filter_all(select(survey,1:22,"SAT_1"), all_vars(. != 999))
# filter_all(select(survey,1:22,"SAT_1"), any_vars(. %in% c(999)))
# 
# filter_all(data_EFA, all_vars(. != 999))
# filter_all(ges, any_vars(. %in% c(999)))
```


```{r}
# delete variables unused in analysis (see case study instructions): 
survey <- survey |> select(-c("C_CR2", "SAT_P1", "SAT_P2", "SAT_P3", "SAT_P4", "SAT_P5", "SAT_P6", "TRU_1", "TRU_2", "TRU_3"))

# replace missing data (999) with NA
survey <- data.frame(sapply(survey,function(x) ifelse((x==999),NA,as.numeric(x))))

# delete missing data
survey <- na.omit(survey)
survey
```



## Explore Data


# Dimensions by which Galeries Lafayette is perceived

## Variable selection

```{r}
data_EFA <- survey[1:22] #same as survey[,1:22]
data_EFA
```

## Exploratory factor analysis


### Check adequacy of correlation Matrix

#### correlation matrix

```{r}
#plot correlation matrix adjusting parameters to see previously identified groupings
corr_matrix <- cor(data_EFA)
corrplot(as.matrix(corr_matrix), 
         method = "shade", 
         order = "hclust", 
         addrect = 10, 
         tl.col ="black", 
         tl.cex = 0.80)
```




#### Bartlett’s Test

```{r}
bart_spher(data_EFA)
```


The Bartlett Test tests the hypothesis that the sample originates from a population, where all variables are uncorrelated. This would not be good for factor analysis, we want this hypothesis to be rejected meaning p-value < 5%. 

In our case we see that it is indeed rejected and that the data is not uncorrelated.

#### KMO


```{r}
KMOTEST=KMOS(data_EFA)
print(KMOTEST, sort=T)
```

The KMO of `r KMOTEST$KMO` is above 0.6 which indicates the data is well suited for factor anlysis.


#### Anti-image Correlation




```{r}
MSA_list <- data.table("Item"=names(KMOTEST$MSA), "MSA"=as.numeric(KMOTEST$MSA))

#Highlighted items that are less than the total KMO value. However many are below this value.
#None are lower than the 0.5 threshold that is indicated in the slides
#Display table
MSA_list<- MSA_list |> 
  setorder(cols = "MSA")
  
MSA_list |> 
  kable() |> 
  kable_minimal() |> 
  row_spec(which(MSA_list[,2]<KMOTEST$KMO), bold = T, color = "white", background = "#78BE20")
```



Variables with MSA values above 0.5 are suited for factor analysis. Presence of items with low MSA’s (<0.5) could also indicate that an important topic hasn’t been well covered in the questionnaire. 

All variables have MSA above 0.5


### PCA

#### Extract factors


```{r}
EFA_PC0 <- psych::principal(data_EFA, rotate="varimax", scores=TRUE)
```


##### Scree plot

```{r}
#display Scree-plot
plot(EFA_PC0$values,xlab="Factor Number",
     ylab="Eigenvalue",
     main="Scree plot",
     cex.lab=1.2,
     cex.axis=1.2,
     cex.main=1.8,
     col = "#0099F8",
     pch = 19) 
abline(h=1, col = "#7F35B2")
```


##### Kaiser Criterion

```{r}
EFA_PC0_kaiser_nb <- length(which(EFA_PC0$values > 1))
EFA_PC0_kaiser_nb
```

The Kaiser criterion suggests we should retain factors with eigenvalues bigger than 1.

There are `r EFA_PC0_kaiser_nb` satisfying this condition.

##### Total Variance Explained

```{r}
#calculate total variance using PAF with nfators = 5
EFA_PC0_EigenValue <- EFA_PC0$values
EFA_PC0_Variance <- EFA_PC0_EigenValue / ncol(data_EFA) * 100
EFA_PC0_SumVariance <- cumsum(EFA_PC0_EigenValue / ncol(data_EFA))
EFA_PC0_Total_Variance_Explained <- cbind("Factor number"=
                                            seq(1, length.out=length(EFA_PC0_EigenValue[EFA_PC0_EigenValue>0])),
                                          EigenValue = EFA_PC0_EigenValue[EFA_PC0_EigenValue>0],
                                          Variance = EFA_PC0_Variance[EFA_PC0_EigenValue>0],
                                          Total_Variance = EFA_PC0_SumVariance[EFA_PC0_EigenValue>0])
#display table
EFA_PC0_Total_Variance_Explained |> 
  kable() |> 
  kable_minimal()
```



With 6 factors we would explain `r EFA_PC0_Total_Variance_Explained[[6,"Total_Variance"]]*100`% of total variance.

With 7 factors we would explain `r EFA_PC0_Total_Variance_Explained[[7,"Total_Variance"]]*100`% of total variance.



```{r}
# test eigenvalue calculation
factorloadings = EFA_PC0$loadings[,1] # loadings 1st factor (default is nfactors = 1)
Eigenvalue = sum(factorloadings^2)
Eigenvalue
```

-----------------------
DELETE?

##### Communalities

```{r}
#plot communalities
EFA_PC0_communalities <- data.frame(sort(EFA_PC0$communality))
EFA_PC0_communalities |> 
  kable() |> 
  kable_minimal() |>
  row_spec(which(EFA_PC0_communalities[,1]<.3), bold = T, color = "white", background = "#78BE20")
```


```{r}
# test communality calculation
variableloading = EFA_PC0$loadings["Im6",] # loadings 1st variable
variableloading
communality = sum(variableloading^2)
communality
```


##### Factor loadings


```{r}
print(EFA_PC0$loadings, cutoff=0.3, sort=TRUE)
```

DELETE?
------------------------------

#### Select number of factors to test

```{r}
# select nb of factors to test
nf = c(5,6,7,8)
```


#### PCA orthogonal Varimax with n factors


```{r}
# perform PCA orthogonal for different numbers of factors
EFA_PCn = list()

i=1
for (n in nf) {
  # EFA_PCn[[i]] <- n
  EFA_PCn[[i]] <- psych::principal(data_EFA, rotate="varimax", scores=TRUE, nfactors = n)
  i=i+1
}
names(EFA_PCn) <- nf

length(EFA_PCn)
```


##### Communalities {.tabset}

```{r, results='asis'}
#communalities for all selected number of factors

for (i in 1:length(nf)) {

  cat("###### Number of factors =", nf[[i]], "{.unnumbered}" ,"\n")
  
    EFA_PCn_communalities <- data.frame(sort(EFA_PCn[[i]]$communality))
    kbl <- EFA_PCn_communalities |> 
              kable() |> 
              kable_minimal() |>
              row_spec(which(EFA_PCn_communalities[,1]<.3), bold = T, color = "white", background = "#78BE20")
    
    print(kbl)
    cat("\n\n")

    # test communality calculation
    variableloading = EFA_PCn[[i]]$loadings["Im9",] # loadings 1st variable
    communality = sum(variableloading^2)
    print(paste0("Communality for Im9 =", communality))
    cat("\n")
    
}
```


##### {.unlisted .unnumbered}


##### Factor loadings {.tabset}

```{r, results='asis'}
# loadings for all selected number of factors

test = list()

for (i in 1:length(nf)) {
  
cat("###### Number of factors =", nf[[i]], "{.unnumbered}" ,"\n")
  
      # print(EFA_PCn[[i]]$loadings, cutoff=0.3)
      print(print_html(model_parameters(EFA_PCn[[i]], loadings=T, threshold = 0.3, summary=T)))
      
cat("\n\n")
}
```

##### {.unlisted .unnumbered}


#### PCA oblique Promax with n factors


```{r}
# perform PCA oblique for different numbers of factors
EFA_PCn_obl = list()

i=1
for (n in nf) {
  # EFA_PCn_obl[[i]] <- n
  EFA_PCn_obl[[i]] <- psych::principal(data_EFA, rotate="promax", scores=TRUE, nfactors = n)
  i=i+1
}
names(EFA_PCn_obl) <- nf

length(EFA_PCn_obl)
```


##### Communalities {.tabset}

```{r, results='asis'}
#communalities for all selected number of factors

for (i in 1:length(nf)) {

  cat("###### Number of factors =", nf[[i]], "{.unnumbered}" ,"\n")
  
    EFA_PCn_obl_communalities <- data.frame(sort(EFA_PCn_obl[[i]]$communality))
    kbl <- EFA_PCn_obl_communalities |> 
              kable() |> 
              kable_minimal() |>
              row_spec(which(EFA_PCn_obl_communalities[,1]<.3), bold = T, color = "white", background = "#78BE20")
    
    print(kbl)
    cat("\n\n")

    # test communality calculation
    variableloading = EFA_PCn_obl[[i]]$loadings["Im9",] # loadings 1st variable
    communality = sum(variableloading^2)
    print(paste0("Communality for Im9 =", communality))
    cat("\n")
    
}
```


##### {.unlisted .unnumbered}


##### Factor loadings {.tabset}

```{r, results='asis'}
# loadings for all selected number of factors

test = list()

for (i in 1:length(nf)) {
  
cat("###### Number of factors =", nf[[i]], "{.unnumbered}" ,"\n")
  
      # print(EFA_PCn_obl[[i]]$loadings, cutoff=0.3)
      print(print_html(model_parameters(EFA_PCn_obl[[i]], loadings=T, threshold = 0.3, summary=T)))
      
cat("\n\n")
}
```



##### {.unlisted .unnumbered}



--------------

## Confirmatory factor analysis

We test whether the constructs found in the exploratory phase adequately describe what is going on.

### First model

```{r}
CFA_model_img <- "
FOOD =~ Im8 + Im10 + Im14
BRAND =~ Im16 + Im17 + Im18 + Im19
CHOICE =~ Im1 + Im2 + Im15
ATMOS =~ Im20 + Im21 + Im22
DECO =~ Im3 + Im4 + Im5
QUAL =~ Im11 + Im12 + Im13
FRENCH =~ Im6 + Im7 + Im9
" 

CFA_fit_img <- cfa(CFA_model_img, data=data_EFA, missing="ML")
summary(CFA_fit_img, fit.measures=TRUE, standardized=TRUE)
```

#### Discussion of global fit measures

Chi square: 
p-value > 0.05

RMSEA
RMSEA <= 0.05 			Good fit
0.05 < RMSEA <= 0.08 		 Acceptable fit
0.08 < RMSEA <= 0.10		 Bad fit
RMSEA > 0.1 			 Unacceptable fit

CFI
CFI < 0.90			definitely reject model
0.90 < CFI < 0.95 		high underrejection rates for misspecified models
CFI > 0.95			accept model

#### local fit measures

```{r, fig.height=8}
# semPaths(CFA_fit_img, what = "path", whatLabels = "std", style = "mx",
#          rotation = 2, layout = "tree3", mar = c(1, 2, 1, 2), 
#          nCharNodes = 7,shapeMan = "rectangle", sizeMan = 8, sizeMan2 = 5, 
#          curvePivot=TRUE, edge.label.cex = 1.2, edge.color = "skyblue4")


semPaths(CFA_fit_img, what = "path", whatLabels = "std", style = "mx",
         rotation = 2, layout = "tree3", mar = c(1, 2, 1, 2), 
         nCharNodes = 7,shapeMan = "rectangle", 
         sizeMan = 4, sizeMan2 = 3, sizeInt = 2, sizeLat = 6, asize = 1.5,
         curvePivot=TRUE, edge.label.cex = .8, edge.color = "skyblue4"
         )
```


```{r}
lambda = inspect(CFA_fit_img, what="std")$lambda
theta = inspect(CFA_fit_img, what="std")$theta

# create lambda matrix with ones instead of std.all
ones <- lambda
ones[ones>0] <- 1

# a matrix with dimensions of lambda matrix but with lambdas replaced by thetas
theta_lb <- theta %*% ones
```

##### Indicator reliability criterion

```{r}
# calculate indicator reliabilities (should be larger than 0.4)
indicrel <- lambda^2/(lambda^2 + theta_lb)
# indicrel

# replace all values satisfying condition with NaN for visibility
indicrel_fail <- indicrel
indicrel_fail[indicrel_fail>.4] <- NaN
indicrel_fail
```

##### Construct reliability criterion

```{r}
# calculate construct reliability (should be above .6)
constrrel <- (t(lambda) %*% ones)^2 / ((t(lambda) %*% ones)^2 + t(theta_lb) %*% ones )
# constrrel

# replace all values satisfying condition with NaN for visibility
constrrel_fail <- constrrel
constrrel_fail[constrrel_fail>.6] <- NaN
constrrel_fail
```

##### Average Variance Extracted criterion

```{r}
# calculate Average Variance Extracted (should be above .5)
AVE <- (t(lambda) %*% lambda) / (t(lambda) %*% lambda + t(theta_lb) %*% ones )
# avgvar

# replace all values satisfying condition with NaN for visibility
AVE_fail <- AVE
AVE_fail[AVE_fail>.5] <- NaN
AVE_fail

```

##### Fornell-Larcker Criteria

```{r}
# correlations between constructs (factors...) should be lower than .7
psi = inspect(CFA_fit_img, what="std")$psi
psi_fail <- psi
psi_fail[psi_fail<.7] <- NaN
psi_fail
```


```{r}
# AVE should be higher than squared correlations between constructs

# replace diagonal of psi matrix with AVE values
psi2 <- psi - psi * diag(1,nrow(psi),ncol(psi)) + diag(AVE) * diag(1,nrow(AVE),ncol(AVE))

# create matrix with columns filled with AVE
AVE_full <- AVE
AVE_full[is.na(AVE_full)] <- 0 #replace NAs with 0s
AVE_full <- AVE_full^0 %*% AVE_full # multiply a matrix full of ones with AVE_full to get columns filled with AVE

# substract matrices any psi bigger than AVE will be negative
AVEpsi_fail <- AVE_full - psi2
# AVE_full - psi2
AVEpsi_fail[AVEpsi_fail >= 0] <- NaN

AVE_full
psi
AVEpsi_fail
```



#### Modification indices

```{r}
arrange(modificationindices(CFA_fit_img),-mi)
```

### Second model

Based on the modification indices we create a new model

```{r}
CFA_model_img <- "
FOOD =~ Im10 + Im14
BRAND =~ Im17 + Im18
PROF =~ Im16 + Im19
CHOICE =~ Im1 + Im2 + Im15
ATMOS =~ Im20 + Im21 + Im22
DECO =~ Im3 + Im4 + Im5
QUAL =~ Im11 + Im12 + Im13
FRENCH =~ Im6 + Im7 + Im8 + Im9
" 

CFA_fit_img <- cfa(CFA_model_img, data=data_EFA, missing="ML")
summary(CFA_fit_img, fit.measures=TRUE, standardized=TRUE)
```

#### Discussion of global fit measures

Chi square: 
p-value > 0.05

RMSEA
RMSEA <= 0.05 			Good fit
0.05 < RMSEA <= 0.08 		 Acceptable fit
0.08 < RMSEA <= 0.10		 Bad fit
RMSEA > 0.1 			 Unacceptable fit

CFI
CFI < 0.90			definitely reject model
0.90 < CFI < 0.95 		high underrejection rates for misspecified models
CFI > 0.95			accept model

#### local fit measures

```{r, fig.height=8}
# semPaths(CFA_fit_img, what = "path", whatLabels = "std", style = "mx",
#          rotation = 2, layout = "tree3", mar = c(1, 2, 1, 2), 
#          nCharNodes = 7,shapeMan = "rectangle", sizeMan = 8, sizeMan2 = 5, 
#          curvePivot=TRUE, edge.label.cex = 1.2, edge.color = "skyblue4")


semPaths(CFA_fit_img, what = "path", whatLabels = "std", style = "mx",
         rotation = 2, layout = "tree3", mar = c(1, 2, 1, 2), 
         nCharNodes = 7,shapeMan = "rectangle", 
         sizeMan = 4, sizeMan2 = 3, sizeInt = 2, sizeLat = 6, asize = 1.5,
         curvePivot=TRUE, edge.label.cex = .8, edge.color = "skyblue4"
         )
```


```{r}
lambda = inspect(CFA_fit_img, what="std")$lambda
theta = inspect(CFA_fit_img, what="std")$theta

# create lambda matrix with ones instead of std.all
ones <- lambda
ones[ones>0] <- 1

# a matrix with dimensions of lambda matrix but with lambdas replaced by thetas
theta_lb <- theta %*% ones
```

##### Indicator reliability criterion

```{r}
# calculate indicator reliabilities (should be larger than 0.4)
indicrel <- lambda^2/(lambda^2 + theta_lb)
# indicrel

# replace all values satisfying condition with NaN for visibility
indicrel_fail <- indicrel
indicrel_fail[indicrel_fail>.4] <- NaN
indicrel_fail
```

##### Construct reliability criterion

```{r}
# calculate construct reliability (should be above .6)
constrrel <- (t(lambda) %*% ones)^2 / ((t(lambda) %*% ones)^2 + t(theta_lb) %*% ones )
# constrrel

# replace all values satisfying condition with NaN for visibility
constrrel_fail <- constrrel
constrrel_fail[constrrel_fail>.6] <- NaN
constrrel_fail
```

##### Average Variance Extracted criterion

```{r}
# calculate Average Variance Extracted (should be above .5)
AVE <- (t(lambda) %*% lambda) / (t(lambda) %*% lambda + t(theta_lb) %*% ones )
# avgvar

# replace all values satisfying condition with NaN for visibility
AVE_fail <- AVE
AVE_fail[AVE_fail>.5] <- NaN
AVE_fail

```

##### Fornell-Larcker Criteria

```{r}
# correlations between constructs (factors...) should be lower than .7
psi = inspect(CFA_fit_img, what="std")$psi
psi_fail <- psi
psi_fail[psi_fail<.7] <- NaN
psi_fail
```


```{r}
# AVE should be higher than squared correlations between constructs

# replace diagonal of psi matrix with AVE values
psi2 <- psi - psi * diag(1,nrow(psi),ncol(psi)) + diag(AVE) * diag(1,nrow(AVE),ncol(AVE))

# create matrix with columns filled with AVE
AVE_full <- AVE
AVE_full[is.na(AVE_full)] <- 0 #replace NAs with 0s
AVE_full <- AVE_full^0 %*% AVE_full # multiply a matrix full of ones with AVE_full to get columns filled with AVE

# substract matrices, replace all values satisfying positive condition (AVE > psi) with NaN
AVEpsi_fail <- AVE_full - psi2
# AVE_full - psi2
AVEpsi_fail[AVEpsi_fail >= 0] <- NaN

AVE_full
psi
AVEpsi_fail
```



#### Modification indices

```{r}
arrange(modificationindices(CFA_fit_img),-mi)
```





--------------------------------------------






### Factor scores for path analysis

```{r}
data_EFA
data.frame(EFA_PCn[[3]]$scores)

CFA_data = cbind(data_EFA, EFA_PCn[[3]]$scores, survey["SAT_1"])
CFA_data
# colnames(CFA_data)[23:29] = c("Gourmet food", "Brand image", "Choice range", "Relaxed atmosphere", "Decoration", "Product quality", "Frenchness")

# colnames(CFA_data)[23:27] = c("FOOD", "BRAND", "CHOICE", "ATMOS", "DECO")
colnames(CFA_data)[23:29] = c("FOOD", "BRAND", "CHOICE", "ATMOS", "DECO", "QUAL", "FRENCH")

CFA_data
```





### model

```{r}
# model_SAT_1 <-"
# SAT_1 ~ Im1 + Im2 + Im3 + Im4 + Im5 + Im6 + Im7 + Im8 + Im9 + Im10 + Im11 + Im12 + Im13 + Im14 + Im15 + Im16 + Im17 + Im18 + Im19 + Im20 + Im21 + Im22
# "

model_SAT_1 <-"
SAT_1 ~ FOOD + BRAND + CHOICE + ATMOS + DECO + QUAL + FRENCH
"

# model_SAT_1 <-"SAT_1 ~ ."
```

### linear regression

```{r}
# linear regression
lm_SAT_1 <-  lm (model_SAT_1, data = CFA_data) 
summary(lm_SAT_1)

# note: lm deletes all missing variables in the Xs! (not in Ys) (see help lavOptions)
```

### path analysis

```{r}
# path analysis
cfa_SAT_1 <- cfa(model_SAT_1, data=CFA_data, missing="ML")
summary(cfa_SAT_1, fit.measures=TRUE, standardized=TRUE)

# note: cfa deletes all missing variables in the Xs! (not in Ys) (see help lavOptions)
```






----------


<!-- # Dimensions by which Galeries Lafayette is perceived -->
<!-- *What are the dimensions by which Galeries Lafayette is perceived? Please explain your findings and rational for your final result.* -->



<!-- ### **Step 1**: Correlation Matrix -->



<!-- ### **Step 2**: Check adequacy of correlation Matrix -->


<!-- #### Bartlett’s Test -->


<!-- #### KMO -->


<!-- #### Anti-image Correlation -->




<!-- ### **Step 3**: choose factor extraction method -->

<!-- #### PCA first iteration full model -->


<!-- #####  **Step 4**: Determine number of components -->

<!-- ###### Scree plot -->

<!-- ###### Kaiser Criterion -->

<!-- ###### Communalities -->

<!-- ###### Total Variance Explained -->

<!-- ###### Factor loadings -->


<!-- #### PCA second iteration -->


<!-- #####  **Step 4**: Determine number of components -->

<!-- ###### Scree plot -->

<!-- ###### Kaiser Criterion -->

<!-- ###### Communalities -->

<!-- ###### Total Variance Explained -->

<!-- ###### Factor loadings -->


<!-- ### **Step 5**: Factor interpretation -->

<!-- #### Oblique rotation and factor pattern matrix -->


<!-- #### Interpretation, dimensions by which Galeries Lafayette is perceived -->


<!-- # Selecting the causal model -->
<!-- *Are the mechanism driving satisfaction and affective commitment similar? Are satisfaction and affective commitment mediating the impact of image perceptions on outcomes? If yes for which outcomes?* -->


<!-- ## Regression with factor scores -->


<!-- ## Mediators -->

<!-- ```{r} -->
<!-- # parameterestimates(fit1, boot.ci.type = "bca.simple", standardized = TRUE)%>% kable() -->
<!-- ``` -->


<!-- # Total effect of image dimensions on outcomes -->
<!-- *What is driving the two distinct outcomes (repurchase and co-creation intention): Please rank the image dimensions with respect to the total effect on each outcome? Interpret your results.* -->




