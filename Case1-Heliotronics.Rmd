---
title: "Case Study 1: Heliotronics Estimating an Experience Curve"
author: "UNIGE - GSEM - Advanced Data-Driven Decision Making"
date: "`r Sys.Date()`"
output: 
  html_document: 
    theme: readable
    highlight: pygments
    toc: true
    toc_float: true
    number_sections: true
---

```{r Setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

```{r cleardata, include=FALSE}
rm(list=ls())
```

> *This analysis was prepared by Francisco Arrieta and Jonathan Edwards.*

------------------------------------------------------------------------

```{r Libraries}
#import necesary libraries
library(data.table)
library(ggplot2)
library(kableExtra)
library(ggfortify)
```

# Manipulating the data


## Data Exploration

Let us first take an overview look at the data

```{r Import Data}
#import data
sol_panels <- fread("Case_Stud_I.csv", sep = ",", header = T)

#display data extract
head(sol_panels) |> 
kbl() |> 
kable_classic_2(full_width = F)

#plot scatter plot of all available data
attach(sol_panels)
plot(number_of_solar_panels, manufacturing_cost, 
     main ="Scatter plot of mean cost per unit", 
     xlab = "Production Amount", 
     ylab = "Manufacturing Cost", 
     pch = 19, 
     col = "#0099F8")
detach(sol_panels)


#show curve with all available data
prod_curve_plot <- ggplot(data = sol_panels, 
                          aes(x= number_of_solar_panels, y = manufacturing_cost)) +
  geom_line(color = "#0099F8")+
  labs(title = "Bevavior of mean cost per unit of production", 
       x= "Production Amount", 
       y = "Manufacturing Cost")+
  theme(plot.title = element_text(color = "#0099F8", size = 12, face = "bold"))

#plot
prod_curve_plot
```

We can see that the relationship is not linear and looks like it might well follow a power law.

## Data transformation

We assume a power law relation between the cost $Y$ of the panels and the number of panels produced $X$:

\begin{aligned}
Y &= AX^{b}\\
\end{aligned}

We can turn this into a linear relationship by taking the log:

\begin{aligned}
\log(Y) &= \log(A) + b\log(X)\\
\end{aligned}

The exponent $b$ becomes the slope of our regression line and $\log(A)$ is the intercept.
Let us plot the log transformed data

```{r Log Transform}
#plot the transformed variables
prod_curve_log <- ggplot(data = sol_panels, aes(x= log(number_of_solar_panels), y = log(manufacturing_cost), color = "Log Transformed")) +
  geom_line() +
  labs(title = "Behavior of mean cost per unit of production", 
       x= "Log transformation of Production Amount", 
       y = "Log transformation of Manufacturing Cost")+
  theme(plot.title = element_text(color = "#0099F8", size = 12, face = "bold"))+
  geom_smooth(method = "lm", 
              formula = y ~ x, 
              se = F,
              size = 0.8,
              aes(color = "Linear Model"))+
scale_color_manual(name = "Regression", values = c("Linear Model" = "#387C2C", "Log Transformed" = "black"))
prod_curve_log

```

After log transformation the data does indeed seem to follow a linear relationship.\


# Fitting a model

## analysis of the regression

Let us analyse our linear regression more quantitatively:

```{r linear model}
#create a linear model to predict experience curve values
y=log(sol_panels$manufacturing_cost)
x=log(sol_panels$number_of_solar_panels)

# lm.out <- lm(y ~ x)
# exp_curve_pred = lm(log(manufacturing_cost) ~log(number_of_solar_panels), data = sol_panels)
exp_curve_pred = lm(y ~ x)

#check assumptions to see if model fits well
plot(exp_curve_pred, which=c(2,1))
summary(exp_curve_pred)

```

The F-statistic is much bigger than 1 which indicates that there is indeed a relationship between our two variables. 

The low p values for the parameter associated to our predictor $\log(number\_of\_solar\_panels)$ indicates that this predictor has a statistically significant relationship with the response $\log(manufacturing\_cost)$

The Residual standard error is small relative to the predicted values (over two orders of magnitude), also the R-squared is very high (over 0.9) which indicates that we have a good fit, the assumed power law realationship is a good one.

The Normal Q-Q plot is straight and of slope 1 indicating that we can assume normality for our variables.

We notice that the data has heteroscedasticity, residuals increase as we move in the future (as manufacturing cost decreases).
This is not good as it means our confidence intervals might be to narrow for the predicted cost of solar panels in the future. We probably would have to go beyond linearity and add higher order terms to our model. But we will not do this here. 

## Learning rate

The learning rate is calculated using the progress ratio. We have

\begin{aligned}
Y &= AX^{b}\\
\tilde Y &= A(2X)^{b}\\
\end{aligned}

By definition, the Progress Ratio is given by
\begin{aligned}
PR &= \frac{\tilde Y}{Y} = \frac{A(2X)^{b}}{AX^{b}}= 2^{b}\\
\end{aligned}

The learning rate (ie proportion of decrease) is:

\begin{aligned}
Learning \ Rate &= \frac{Y - \tilde Y}{Y} = 1- \frac{\tilde Y}{Y} = 1 - PR= 1 - 2^{b}\\
\end{aligned}

We can calculate $A$ and $b$ from the parameters obtained through the linear fit of the log transformed data 

```{r calculate A and B}
# get A from log-log fit intercept
log_A <- exp_curve_pred$coefficients[1]
A <- as.numeric(exp(log_A))
paste("A =",A)

#get b from log-log slope
b <- as.numeric(exp_curve_pred$coefficients[2])
paste("b =",b)
```

With these values we now compute the progress ratio and learning rate

```{r learning rate}
#progress ratio
prog_ratio = 2^b
paste("The progress ratio is:",prog_ratio)

#learning rate
learn_rate = 1-prog_ratio 
paste("The learning rate is:", learn_rate)
```

The learning rate of `r round(learn_rate*100,2)`%  in this product group seems to be quite low relative to other product groups, it is comparable to that of electric stoves, 

# Predicting Price 

## create predicted data

```{r create predicted data}
#create table with units up to 5000 cumulative
pred_cost_table <- data.frame("Production_Amount" = seq(100, 5000, 100))

#add column with predicted value
pred_cost_table["Pred_Cost"] <- (pred_cost_table$Production_Amount^b)*A
```



## estimate the cost


Let us start by plotting the prediction curve on the original, non log transformed data:

```{r plot predicted data}
#create a plot to see real data vs prediction
pred_cost_plot <- ggplot() +
  geom_point(data = sol_panels, 
             aes(x= number_of_solar_panels, 
                 y = manufacturing_cost), 
             color = "#0099F8") +
  geom_line(data = pred_cost_table, 
            aes(x= Production_Amount, 
                y = Pred_Cost), 
            color = "#7F35B2", 
            size = 1)+
  labs(title = "Prediction of mean cost per unit of production", 
       x= "Production Amount", 
       y = "Manufacturing Cost")+
  theme(plot.title = element_text(color = "#0099F8", size = 12, face = "bold"))
  
pred_cost_plot

```


By the time production of panels for the Tessin project starts it is estimated that a cumulative amount of 4600 panels will have been produced. To estimate the cost of the 400 panels to be produced for Tessin, we want to take the mean of the predicted cost for the batches of 100 panels corresponding to these panels: 4700, 4800, 4900 and 5000.

Here are the predicted costs for these batches:

```{r show data last 4 periods}
#show last 4 periods
pred_cost_table[47:50,]
```

We calculate their mean:

```{r}
mean_cost <- mean(pred_cost_table[47:50, "Pred_Cost"])
paste("mean cost= ",mean_cost)
```


# Predicting Confidence Interval

We now want to be able to give a confidence interval on this mean predicted price.

## Method 1: using parameter confidence interval

In this first method, first we extract the 95% confidence interval estimate for experience parameter $b$.
We then use the upper and lower bounds of experience parameter $b$'s confidence interval to calculate upper and lower bound predictions. Note that we keep the intercept fixed and do not take into account its confidence interval.

```{r ci with method one}
#calculate confidence intervals
CI = confint(exp_curve_pred)

b_upper = CI[2,2]
b_lower = CI[2,1]

#compare with standard deviation
CI_int=b_upper-b
std = 0.007936
CI_std = 0.007936*1.96
paste("Confidence interval half size=",CI_int)
paste("Confidence interval half size from std error=",CI_std)

# ((b_upper-b_lower)/2)^2
# as.numeric(b_upper-b)^(1/2)

#add individual CI per prod level
pred_cost_table["Upper_Bound"] <- A*(pred_cost_table$Production_Amount^b_upper)
pred_cost_table["Lower_Bound"] <- A*(pred_cost_table$Production_Amount^b_lower)

#plot CI regressions in same graph
pred_cost_plot_CI <- 
  pred_cost_plot + 
  geom_line(data = pred_cost_table,
            aes(x= Production_Amount, y = Upper_Bound, color = "Experience Parameter Bounds"))+
  geom_line(data = pred_cost_table,
            aes(x= Production_Amount, y = Lower_Bound, color = "Experience Parameter Bounds"))+
  scale_color_manual(name = "Intervals", 
                     values = c("Experience Parameter Bounds" = "#00C1D5"))
pred_cost_plot_CI
```

```{r summary table}
#create summary of average values for last 400 units
avg_values <- data.frame("Avg Lower Bound" = round(mean(pred_cost_table[47:50,4]),2),
                        "Avg Production" = round(mean(pred_cost_table[47:50,2]),2),
                        "Avg Upper Bound" = round(mean(pred_cost_table[47:50,3]),2))

#display data extract
avg_values |> 
kbl() |> 
kable_classic_2(full_width = F)
```


## Method 2: using confidence bands

What we are really interested in is the confidence interval on the predictions, not on the parameters.
There is no immediate way to obtain the former from the latter.
But we can in fact also extract the predicted variable confidence intervals from the model directly and use this.
For information purposes we have also provided prediction interval curves and results

```{r ci with method 2}
# Predict confidence interval (and prediction interval)

newx = seq(100,5000,by = 100)

pred_int_pred <- predict(exp_curve_pred, newdata=data.frame(x=log(newx)), interval="prediction",
                         level = 0.95)
conf_int_pred <- predict(exp_curve_pred, newdata=data.frame(x=log(newx)), interval="confidence",
                         level = 0.95)

# summary(exp_curve_pred)
summary(conf_int_pred)
```

```{r plot all bounds}
# log transformed data plot

ggplot() +
  geom_point(data=sol_panels,
             aes(x= log(number_of_solar_panels),
                 y = log(manufacturing_cost)),
             color = "#0099F8") +
  geom_line(data=conf_int_pred,
            aes(x=log(newx),y=fit),color="#7F35B2", 
            size = 1)+
  #upper Experience parameter bound
  geom_line(data = pred_cost_table,
            aes(x= log(Production_Amount), y = log(Upper_Bound), color = "Experience Parameter Bounds"))+
  #lower Experience parameter bound
  geom_line(data = pred_cost_table,
            aes(x= log(Production_Amount), y = log(Lower_Bound), color = "Experience Parameter Bounds"))+
  #lower confidence interval
  geom_line(
            aes(x= log(newx),
                y = conf_int_pred[,2], color = "Confidence Intervals")) +
  #upper confidence interval
  geom_line(
            aes(x= log(newx),
                y = conf_int_pred[,3], color = "Confidence Intervals")) +
  #lower pred interval
  geom_line(
            aes(x= log(newx),
                y = pred_int_pred[,2], color = "Prediction Intervals")) +
  #upper pred interval
  geom_line(
            aes(x= log(newx),
                y = pred_int_pred[,3], color = "Prediction Intervals")) +
  labs(title = "Log-log plot of Prediction of mean cost per unit of production",
       x= "log(Production Amount)",
       y = "log(Manufacturing Cost)")+
  theme(plot.title = element_text(color = "#0099F8", size = 12, face = "bold")) +
  scale_color_manual(name = "Intervals", 
                     values = c("Experience Parameter Bounds" = "#00C1D5", "Confidence Intervals" = "lightpink", "Prediction Intervals"="lightgrey"))

# non transformed data plot
ggplot() +
  geom_point(data=sol_panels,
             aes(x= number_of_solar_panels,
                 y = manufacturing_cost),
             color = "#0099F8") +
  geom_line(data=conf_int_pred,
            aes(x=newx,y=exp(fit)),color="#7F35B2", 
            size = 1)+
  #upper Experience parameter bound
  geom_line(data = pred_cost_table,
            aes(x= Production_Amount, y = Upper_Bound, color = "Experience Parameter Bounds"))+
  #lower Experience parameter bound
  geom_line(data = pred_cost_table,
            aes(x= Production_Amount, y = Lower_Bound, color = "Experience Parameter Bounds"))+
  #lower confidence interval
  geom_line(
            aes(x= newx,
                y = exp(conf_int_pred[,2]), color = "Confidence Intervals")) +
  #upper confidence interval
  geom_line(
            aes(x= newx,
                y = exp(conf_int_pred[,3]), color = "Confidence Intervals")) +
  #lower pred interval
  geom_line(
            aes(x= newx,
                y = exp(pred_int_pred[,2]), color = "Prediction Intervals")) +
  #upper pred interval
  geom_line(
            aes(x= newx,
                y = exp(pred_int_pred[,3]), color = "Prediction Intervals")) +
  labs(title = "Plot of Prediction of mean cost per unit of production",
       x= "log(Production Amount)",
       y = "log(Manufacturing Cost)")+
  theme(plot.title = element_text(color = "#0099F8", size = 12, face = "bold")) +
  scale_color_manual(name = "Intervals", 
                     values = c("Experience Parameter Bounds" = "#00C1D5", "Confidence Intervals" = "lightpink", "Prediction Intervals"="lightgrey"))

```


Below is a summary of the intervals calculated with the 3 methods

```{r table for all results}
#create summary of average values for last 400 units using confidence interval
avg_values <- data.frame("Interval type" = c("Experience Parameter Bounds","Confidence Interval",
                                             "Prediction Interval"),
                        "Avg Lower Bound" = c(round(mean(pred_cost_table[47:50,4]),2),
                                              round(mean(exp(conf_int_pred[47:50,2]),2)),
                                              round(mean(exp(pred_int_pred[47:50,2]),2))),
                        "Avg Production" = c(round(mean(pred_cost_table[47:50,2]),2),
                                             round(mean(exp(conf_int_pred[47:50,1]),2)),
                                             round(mean(exp(pred_int_pred[47:50,1]),2))),
                        "Avg Upper Bound" = c(round(mean(pred_cost_table[47:50,3]),2),
                                              round(mean(exp(conf_int_pred[47:50,3]),2)),
                                              round(mean(exp(pred_int_pred[47:50,3]),2))))


#display data extract
avg_values |> 
kbl() |> 
kable_classic_2(full_width = F)
```

We notice that the confidence intervals are the smallest and that the intervals used in method 1 are the largest.
in this particular case we would have to be careful with this result due to heteroscedasticity of the fit, and probably consider slightly larger intervals.
We can also note that in all cases this analysis is useful because the intervals do not contain the current cost.
