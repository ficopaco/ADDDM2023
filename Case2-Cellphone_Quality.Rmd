---
title: "Case Study 2: Heliotronics Estimating an Experience Curve"
author: "UNIGE - GSEM - Advanced Data-Driven Decision Making"
date: "`r Sys.Date()`"
output: 
  html_document: 
    theme: readable
    highlight: pygments
    toc: true
    toc_float: true
    number_sections: true
---

```{r Setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

> *This analysis was prepared by Francisco Arrieta and Jonathan Edwards.*

------------------------------------------------------------------------

```{r Load Libraries}
library(data.table)
library(ggplot2)
library(corrplot) #correlation matrix plot
library(olsrr)  #VIF and Tolerance Values
library(dplyr)
library(rcompanion) #histograms with normal curve
library(REdaS) #KMO and Bartlett test
library(psych)
library(gridExtra)
library(kableExtra)
library(caret)
```

# Data Exploration

```{r Import and Clean data}
#import data
cel_data <- fread("Data File_Case_Study_Factor Analysis_MD.csv", sep = ",", header = T)

#explore data structure
str(cel_data)
summary(cel_data)

#clean data table
cel_data <- cel_data[, -c("ownership", "intro_q", "intro_b", "intro_d", "defect1")]
cel_data <- cel_data[, -c(57:64)]
cel_data <- na.omit(cel_data)

#create data table only for questions
cel_data_quests <- cel_data[, 7:39]
```


```{r Histograms}
#plot histograms of certain variables
par(mfrow = c(2, 3))
plotNormalHistogram(cel_data$qd1, main = paste("Distribution of QD1"), xlab = "Values", col = "#0099F8")
plotNormalHistogram(cel_data$qd23, main = paste("istribution of QD23"), xlab = "Values", col = "#0099F8")
plotNormalHistogram(cel_data$wtp1, main = paste("Distribution of WTP1"), xlab = "Values", col = "#0099F8")
plotNormalHistogram(cel_data$ri1, main = paste("Distribution of RI1"), xlab = "Values", col = "#0099F8")
plotNormalHistogram(cel_data$brandrec, main = paste("Distribution of BRANDREC"), xlab = "Values", col = "#0099F8")
```



```{r Correlation matrix}
#plot correlation matrix adjusting parameters to see previously identified groupings
corr_matrix <- cor(cel_data_quests)
corrplot(as.matrix(corr_matrix), 
         method = "shade", 
         order = "hclust", 
         addrect = 10, 
         tl.col ="black", 
         tl.cex = 0.80)

```

```{r KMO and Bartlett}
#calculate KMO and MSA errors
KMOTEST <- KMOS(cel_data_quests)
KMOTEST

KMO_val <- KMOTEST$KMO
KMO_val

#loop to extract and create data table
KMO_list <- data.table("Item" = character(), "Score" = double())
for(i in 1:33){
  KMO_list<- rbindlist(list(KMO_list, list(paste("qd",i, sep=""), KMOTEST$MSA[i])))
}

#Highlighted items that are less than the total KMO value. However many are below this value.
#None are lower than the 0.4 threshold that is indicated in the slides
#Display table
KMO_list |> 
  setorder(cols = "Score") |> 
  kable() |> 
  kable_minimal() |> 
  row_spec(which(KMO_list[,2]<KMO_val), bold = T, color = "white", background = "#78BE20")

#check Bartlett-Test
bart_spher(cel_data_quests)
# this tests that there is enough correlation to be able to create factors.
#Null hypothesis is that there is no correlation (all are uncorrelated) and the pvalue below 0.05 indicates that they are


#mention how the first two are lower than the rest but not particularly with regards to thresholds seen in class.
```

# Principal Axis Factoring


```{r PAF Best models}
#tried to find the best amount of factors in PAF.
#Always around 5 which is lower than the ideal 8.

#find best combination of factors
best_factors <- data.table("Factors" = double(), "Amount" = double())
best_eigen <- data.table("Index" = 1:31)

for(i in 1:31){
  temp_paf <- psych::fa(data.table(scale(cel_data_quests[,-c(4,23)])), nfactors = i, rotate = "varimax", scores = T)
  fact_len <- length(which(temp_paf$values > 1))
  best_eigen <- best_eigen[,paste("Factor_", i, sep=""):= temp_paf$values]
  best_factors <- rbindlist(list(best_factors,list(i, fact_len)))
}

#plot the amount of factors with eigen value >1 for each nfactors up to 33
ggplot(best_factors, aes(Factors,Amount))+
  geom_line(color = "#0099F8", size = 0.5)+
  ggtitle("Eigen Value behavior by Model")+
  labs(xlab="Model", ylab = "Amount of factors")+
  theme_classic()

#Plot the behavior of eigen values per amount of factors per model
best_eigen_melt <- melt(best_eigen ,  id.vars = "Index", variable.name = "Series")
ggplot(best_eigen_melt, aes(Index, value))+
  geom_line(aes(color = Series))+
  xlim(0,12)+
  geom_hline(yintercept=1)+
  ggtitle("Amount of Factors with Eigen Value above 1")+
  labs(xlab="Amount of Factors", ylab = "Eigen Value")+
  theme_classic()

#calculate the eigen values with best number of factors
paf1 <- psych::fa(cel_data_quests, nfactors = 5, rotate = "Varimax", scores = T)
summary(paf1)

#display Scree-plot
plot(paf1$values,
     xlab="Factor Number",
     ylab="Eigenvalue",
     main="Scree plot",
     cex.lab=1.2,
     cex.axis=1.2,
     cex.main=1.8,
     col = "#0099F8",
     pch = 19) +abline(h=1, col = "#7F35B2")

#PAF always suggests 5 factors. However based on how they line up it does not make logical sense. 
```



```{r PAF Total Variance}
#calculate total variance using PAF with nfators = 5
pafEigenValue <- paf1$values
pafVariance <- pafEigenValue / ncol(cel_data_quests) * 100
pafSumVariance <- cumsum(pafEigenValue / ncol(cel_data_quests))
pafTotal_Variance_Explained <- cbind(EigenValue = pafEigenValue[pafEigenValue>0],
                                  Variance = pafVariance[pafEigenValue>0],
                                  Total_Variance = pafSumVariance[pafEigenValue>0])
#display table
pafTotal_Variance_Explained |> 
  kable() |> 
  kable_minimal()
```


```{r PAF Loadings}
#Show lodings for PAF using nfactors = 5
print(paf1$loadings, cutoff=0.4, sort=TRUE)

#Talk about groupings not being precise with assignment from groups in the PDF.
#Mention the variables being predicted by multiple variables
#mention the need to improve the groupings
```



# Principal Component Analysis

```{r PC analysis}
#find best combination of factors
best_factors_pc <- data.table("Factors" = double(), "Amount" = double())
best_eigen_pc <- data.table("Index" = 1:33)

for(i in 1:33){
  temp_paf_pc <- psych::principal(data.table(scale(cel_data_quests)), nfactors = i, rotate = "varimax", scores = T)
  fact_len_pc <- length(which(temp_paf_pc$values > 1))
  best_eigen_pc <- best_eigen_pc[,paste("Factor_", i, sep=""):= temp_paf_pc$values]
  best_factors_pc <- rbindlist(list(best_factors_pc,list(i, fact_len_pc)))
}

#plot the amount of factors with eigen value >1 for each nfactors up to 33
best_factors_plot <- ggplot(best_factors_pc, aes(Factors,Amount))+
  geom_line(color = "#0099F8", size = 0.5)+
  ggtitle("Eigen Value behavior by Model")+
  labs(xlab="Model", ylab = "Amount of factors")+
  theme_classic()
best_factors_plot

#calculate the eigen values with best number of factors
best_eigen_melt <- melt(best_eigen_pc ,  id.vars = "Index", variable.name = "Series")
best_eigen_plot <- ggplot(best_eigen_melt, aes(Index, value))+
  geom_line(aes(color = Series))+
  xlim(0,12)+
  geom_hline(yintercept=1)+
  ggtitle("Amount of Factors with Eigen Value above 1")+
  labs(xlab="Amount of Factors", ylab = "Eigen Value")+
  theme_classic()
best_eigen_plot


#they all produce 8 groups. Eigen values are the same for every group. with or without qd4 and qd23
```


```{r PC analysis 1}
#calculate the eigen values with best number of factors
cd_PC1 <- psych::principal(cel_data_quests[,-c(4,23)], nfactors = 8, rotate="varimax", scores=TRUE)
length(which(cd_PC1$values >1))

#display Scree-plot
plot(cd_PC1$values,xlab="Factor Number",
     ylab="Eigenvalue",
     main="Scree plot",
     cex.lab=1.2,
     cex.axis=1.2,
     cex.main=1.8,
     col = "#0099F8",
     pch = 19) +abline(h=1, col = "#7F35B2")

#comment on how without qd4 and qd23 the ideal eigen value is 7 factors
```



```{r PCA Total Variance}
#calculate total variance using PAF with nfactors = 8
EigenValue <- cd_PC1$values
Variance <- EigenValue / ncol(cel_data_quests) * 100
SumVariance <- cumsum(EigenValue / ncol(cel_data_quests))
Total_Variance_Explained <- cbind(EigenValue = EigenValue[EigenValue>0],
                                  Variance = Variance[EigenValue>0],
                                  Total_Variance = SumVariance[EigenValue>0])
#display table
Total_Variance_Explained |> 
  kable() |> 
  kable_minimal()
```

```{r PC analysis 2}

PC1_communalities <- data.frame(sort(cd_PC1$communality))
PC1_communalities |> 
  kable() |> 
  kable_minimal()
  
#qd23 has the lowest communality but it is not terribly far away from the other values
#qd4 is low but not the worst


pc_loads <- print(cd_PC1$loadings, cutoff=0.4, sort=TRUE)

```




```{r Regression and factor scores}
cd_factors <- cbind(cel_data_quests,cd_PC1$scores)
colnames(cd_factors)[34:41]=c("Serviceability",
                          "Performance",
                          "Durability",
                          "Aesthetics",
                          "Ease_of_use",
                          "Features",
                          "Reliability",
                          "Prestige")
```

# Oblique Rotation

```{r Oblique Rotation}

cd_oblique <- na.omit(cel_data[, c(43:45, 51:52)])
#cd_oblique2 <- cel_data[,c(9:41, 43:45, 51:52)]

cd_oblq1 <- psych::principal(cel_data_quests, rotate="promax",nfactors=8, scores=TRUE)
cd_oblq3 <- psych::principal(cel_data_quests[,-c(4,23)], rotate="promax",nfactors=8, scores=TRUE)
cd_oblq2 <- psych::principal(cd_oblique, rotate="promax",nfactors=2, scores=TRUE)


print(cd_oblq3$loadings, cutoff=0.4, sort=TRUE)


cd_oblq1_communalities <- data.frame(sort(cd_oblq1$communality))
cd_oblq1_communalities
#qd3 has the lowest communality but it is not terribly far away from the other values

plot(cd_oblq1$values,xlab="Factor Number",
     ylab="Eigenvalue",
     main="Scree plot",
     cex.lab=1.2,
     cex.axis=1.2,
     cex.main=1.8) +abline(h=1)

print(cd_oblq1$loadings, cutoff=0.4, sort=TRUE)


pred_table <- data.frame(cd_oblq1$scores)
pred_table <- cbind(pred_table,cd_oblq2$scores)


pred_table2 <- data.frame(cd_oblq3$scores)
pred_table2 <- cbind(pred_table,cd_oblq2$scores)
pred_table2 <- cbind(pred_table,cel_data_quests[,c(4,23)])


colnames(pred_table)[1:10]=c("Serviceability",
                          "Performance",
                          "Durability",
                          "Aesthetics",
                          "Ease_of_use",
                          "Features",
                          "Reliability",
                          "Prestige",
                          "Willingness_to_pay",
                          "Repurchase_Intention")

colnames(pred_table2)[1:10]=c("Serviceability",
                          "Performance",
                          "Features",
                          "Aesthetics",
                          "Ease_of_use",
                          "Durability",
                          "Reliability",
                          "Conformance",
                          "Willingness_to_pay",
                          "Repurchase_Intention")

```

```{r Correlation of Factors}
pred_corr_matrix <- cor(pred_table)
corrplot(as.matrix(pred_corr_matrix))

pred_corr_matrix <- cor(pred_table2)
corrplot(as.matrix(pred_corr_matrix))
```

```{r}
hist(pred_table$Willingness_to_pay)
hist(pred_table$Repurchase_Intention)
summary(pred_table$Willingness_to_pay)
summary(pred_table$Repurchase_Intention)

```


# Regression Analysis

```{r Regression Models}
pred_table <- as.data.table(scale(pred_table))
pred_table2 <- as.data.table(scale(pred_table2))

wtp_model <- lm(Willingness_to_pay ~ Serviceability +Performance + Durability + Aesthetics + Ease_of_use + Features + Reliability + Prestige, data = pred_table)

wtp_model3 <- lm(Willingness_to_pay ~ Serviceability +Performance + Durability + Aesthetics + Ease_of_use + Features + Reliability + Conformance + qd4 + qd23, data = pred_table2)

wtp_model4 <- lm(Willingness_to_pay ~ Serviceability +Performance + Durability + Aesthetics + Ease_of_use + Features + Reliability, data = pred_table2)

wtp_model2 <- lm(Willingness_to_pay ~ Serviceability +Performance + Durability + Aesthetics + Ease_of_use + Features + Reliability + Conformance, data = pred_table2)

summary(wtp_model)
summary(wtp_model2)
summary(wtp_model3)
summary(wtp_model4)

ri_model <- lm(Repurchase_Intention ~ Serviceability +Performance + Durability + Aesthetics + Ease_of_use + Features + Reliability + Prestige, data = pred_table)

ri_model3 <- lm(Repurchase_Intention ~ Serviceability +Performance + Durability + Aesthetics + Ease_of_use + Features + Reliability + Conformance + qd4 + qd23, data = pred_table2)

ri_model4 <- lm(Repurchase_Intention ~ Serviceability +Performance + Durability + Aesthetics + Ease_of_use + Features + Reliability, data = pred_table2)

ri_model2 <- lm(Repurchase_Intention ~ Serviceability +Performance + Durability + Aesthetics + Ease_of_use + Features + Reliability + Conformance, data = pred_table2)

summary(ri_model)
summary(ri_model2)
summary(ri_model3)
summary(ri_model4)
```




```{r}
# #dummy variables must be created manually, hence convert to factors
# ucExtract$Fuel_Type <- as.factor(usedCars$Fuel_Type)
# 
# #create dummy variables list
# ucExtDummies <- dummyVars(~ ., data=ucExtract)
# 
# #create new table with dummy variables
# ucExtract <- as.data.frame(predict(ucExtDummies, newdata = usedCars))

cel_data$brandrec <- as.factor(cel_data$brandrec)
str(cel_data$brandrec)

dummies <- dummyVars(~., data = cel_data)

Ext_cel_data <- as.data.frame(predict(dummies, newdata = cel_data))

pred_table <- cbind(pred_table, Ext_cel_data[,61:65])
pred_table2 <- cbind(pred_table, Ext_cel_data[,61:65])


```

```{r}
pred_table_1 <- pred_table[brandrec.1==1,]
pred_table_2 <- pred_table[brandrec.2 ==1]
pred_table_3 <- pred_table[brandrec.3 ==1]
pred_table_4 <- pred_table[brandrec.4 ==1]
pred_table_5 <- pred_table[brandrec.5 ==1]

wtp_model <- lm(Willingness_to_pay ~ Serviceability +Performance + Durability + Aesthetics + Ease_of_use + Features + Reliability + Prestige, data = pred_table_1)

wtp_model2 <- lm(Willingness_to_pay ~ Serviceability +Performance + Durability + Aesthetics + Ease_of_use + Features + Reliability + Prestige, data = pred_table_2)

wtp_model3 <- lm(Willingness_to_pay ~ Serviceability +Performance + Durability + Aesthetics + Ease_of_use + Features + Reliability + Prestige, data = pred_table_3)

wtp_model4 <- lm(Willingness_to_pay ~ Serviceability +Performance + Durability + Aesthetics + Ease_of_use + Features + Reliability + Prestige, data = pred_table_4)

wtp_model5 <- lm(Willingness_to_pay ~ Serviceability +Performance + Durability + Aesthetics + Ease_of_use + Features + Reliability + Prestige, data = pred_table_5)

summary(wtp_model)
summary(wtp_model2)
summary(wtp_model3)
summary(wtp_model4)
summary(wtp_model5)
```



```{r}
#create subset of data per brand
#for (i in 1:5){
  #nam <- paste("cd_", as.character(i), sep = "")
  #assign(nam, cel_data[cel_data$brandrec == i,])
  
  #print(as.data.table(nam))
  #xx <- subset(as.data.table(nam), brandrec == i)
  
  #separate the variables
  #temp_quest <- psych::principal(nam, rotate="promax",nfactors=8, scores=TRUE)
  #temp_vars <- psych::principal(nam, rotate="promax",nfactors=2, scores=TRUE)
#}

#xx <- lm(ri_1~ri_2 + brandrec=1, data = cel_data)


```


